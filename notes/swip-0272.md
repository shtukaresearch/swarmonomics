---
title: Stake registry update queue
author: Andrew Macpherson (@awmacpherson)
discussions-to: https://discord.gg/Q6BvSkCv (Swarm Discord)
status: WIP
category: Core
created: 2025-10-10
---

# Stake registry update queue

## Abstract

Introduce a FIFO queue that adds controlled delays to all updates to stake balances and metadata (i.e. height and overlay address).

## Motivation

Currently, the following operations are possible on a stake balance:

* Create/destroy.
* Deposit/withdraw.
* Update height (up/down).
* Update overlay address.

Some of these operations — for example, height reduction — result in a node's exit from or reduced financial commitment to a neighbourhood, negatively affecting the storage service. For the sake of service stability, we propose a method to throttle such operations by means of an update queue which imposes additional delays on service-reducing changes. 

The 2 round thaw currently imposed on stakers after any change to participation metadata is absorbed into the logic of this queue. Instead of being blocked from participation, possibly leading to service disruption, stakers can continue to play using their existing stake metadata during the 2 round delay.

Requests to update registered information (committed stake, height, and overlay address) are placed on a FIFO queue, maintained by a new UpdateQueue subsystem, and executed lazily on calls to getter methods in the Stake Registry contract. An update is not processed unless a certain number of complete rounds has elapsed since it was placed on the queue. The number of rounds depends on the nature of the update; updates that allow stake balances to be reduced or nodes to remove neighbourhoods from their area of responsibility are subjected to longer delays.

## Specification

### Architecture

The proposal calls for the deployment of a new UpdateQueue contract that is authorised to make certain permissioned calls into the Stake Registry. The Stake Registry maintains a reference to the UpdateQueue contract so that it can push requests to and pop from the queue. All other interactions with the UpdateQueue contract are mediated through the Stake Registry.

### Parameters

| Name                      | Value  | Description                                                  |
| ------------------------- | ------ | ------------------------------------------------------------ |
| `EXIT_DELAY`              | `3184` | Minimum delay in rounds to impose for height reduction       |
| `OVERLAY_CHANGE_DELAY`    | `796`  | Minimum delay in rounds to impose for change of overlay address. |
| `BASE_UPDATE_DELAY`       | `2`    | Minimum delay in rounds to impose for all operations.        |
| `UPDATE_QUEUE_MAX_LENGTH` | `10`   | Maximum number of pending request items per owner.           |

Malicious changes to these variables could have the effect of trapping nodes indefinitely, so we propose that their values be embedded into the UpdateQueue contract at deployment time and not be modifiable by the admin.

### StakeRegistry

* Updates that would have been managed by calling `StakeRegistry:manageStake` are added to a request queue and triggered lazily after a delay, which is determined by the queue subsystem, instead of executing instantly. The type of delay to impose depends on the update and is decided at the time of adding the update to the queue.

  The delay to impose is decided as follows:

  1. If `height` is reduced, apply `ExitDelay`.
  2. Otherwise, if overlay address is changed, apply `OverlayChangeDelay`.
  3. Otherwise apply `BaseDelay`.

  A new access-controlled call `StakeRegistry:_manageStake` that can only be called by the UpdateQueue contract must be implemented to actually apply the updates.

* Calls to `StakeRegistry:migrateStake` clear the queue and withdraw immediately.

* The logic of `withdrawFromStake` must define `_surplusStake` using the look-ahead `updateQueueCommittedStake(msg.sender)` method of the update queue rather than the entry in the current stake table.

* `lastUpdatedBlockNumber` is now only used for freezing.

* The `Redistribution.commit()` method wants to pop all valid updates from the queue before getting the values of the view functions `overlayOfAddress()`, `nodeEffectiveStake()`, `lastUpdatedBlockNumberOfAddress()`, and `heightOfAddress()`. Should it do this explicitly, or should it be baked into the definition of those functions (which breaks the `view` property)? 

  I think we need a two sets of endpoints: `view` endpoints that apply pending updates in memory but do not modify state, and endpoints with side-effects that apply the updates to state.

#### Tests

* The Stake Registry cannot report stale values. All `view` endpoints as well as `updateAndGet*` endpoints
* Calls to `manageStake` should be enqueued in order and with correct delays.
  * A "trivial" update with `_setNonce` and `height` the same as before and `_addAmount = 0` receives the Base Delay. Trivial updates may still result in a change to `committedStake`. More generally, any update which leaves `_setNonce` unchanged and does not decrease `height` receives Base Delay.
  * Any update that decreases `height` receives the Exit Delay, regardless of what other changes it makes.
  * An update that changes `_setNonce` and either leaves `height` the same or increases it receives the Overlay Change Delay.
  * FIFO structure: if a `height` decrease is enqueued, then an update with a short delay is enqueued, the second update is not applied until after the full `EXIT_DELAY`.
* Call to `migrateStake` correctly processes enqueued deposit liabilities and withdraws all deposited tokens (when paused).
* Call to `withdrawFromStake` takes enqueued stake commitment into account when computing stake surplus. If there is some surplus, enqueuing another update makes that surplus instantly committed and inaccessible to `withdrawFromStake`.

#### Sample implementation

```solidity
enum delay_t { BaseDelay, ExitDelay, OverlayChangeDelay }

interface IUpdateQueue {

}

function manageStake(bytes32 _setNonce, uint256 _addAmount, uint8 _height) external whenNotPaused {
      // As in v0.9.3 except with the following section removed:
      /*  
          stakes[msg.sender] = Stake({
              overlay: _newOverlay,
              committedStake: updatedCommittedStake,
              potentialStake: updatedPotentialStake,
              lastUpdatedBlockNumber: block.number,
              height: _height
          });
      */ 
      // and the following added:
      
      // Select delay type based on nature of update
      delay_t delay
      if (_height < stakes[msg.sender].height)
          delay = ExitDelay
      else if (_newOverlay != _previousOverlay)
          delay = OverlayChangeDelay
      else
          delay = BaseDelay
      
      // Encode call and push to queue with configured delay
      bytes request = abi.encodeCall(
          _updateStake, (msg.sender, _newOverlay, updatedCommittedStake, updatedPotentialStake, block.number, _height)
      )
      UpdateQueue.push(msg.sender, request, delay)
  }
      
 // New access control role: Updater
 // 	grant to UpdateQueue contract
 // New modifier: onlyUpdater
 
  function _updateStake onlyUpdater (
      address owner, 
      bytes32 overlay, 
      uint committedStake, 
      uint potentialStake, 
      uint blockNumber,
      uint8 height
  ) {
      stakes[owner] = Stake({
          overlay: overlay,
          committedStake: committedStake,
          potentialStake: potentialStake,
          lastUpdatedBlockNumber: blockNumber,
          height: height
      })
  }
  
  function applyUpdates(address owner) {
      UpdateQueue.applyUpdates(owner)
  }
```

#### Redistribution

* The following calls may need to check the queue state:

  * `Redistribution:commit()`. Pop and process all updates with `Pending` status in the queue before proceeding with the commit. The call to Pop occurs *after* the check for frozen status, implying that updates are not processed if the node is frozen. (This matters if the updates also set `lastUpdatedBlockNumber`.)
  * `Redistribution:reveal()`. The counter that tracks reveals for the purposes of updating prices may need an adjustment for nodes currently waiting to leave the neighbourhood.
* `Redistribution:commit()` currently checks the `lastUpdatedBlockNumber` to see if a node can participate. This is used in freezing as well as the post-update thaw.

**Tests.**

* `commit()` does not access stale state. Pending updates are always applied before values are used.
* If less than 2 rounds have elapsed since any update was enqueued, `commit()` sees the same metadata as before. Therefore the same `obfuscatedHash` should be usable.

#### UpdateQueue

The queue is FCFS with delays of various lengths classified by an enum:

* `ExitDelay` 
* `OverlayChangeDelay`
* `BaseDelay`

The delay can be thought of as a "not valid before" timer. An update is not necessarily applicable immediately after this delay expires: due to FCFS ordering, it could be held up by another request higher up in the queue with a longer delay.

The update is recorded as a struct comprising the new balance, overlay address, and height that would be effected by the update. 

Queue must implement the following methods:

* `push(address owner, bytes encodedCall, delay_t delay)`. Add update `encodedCall` to the queue of updates for `owner`.

  ```
  1. uint delay = DELAY_PARAMETERS[delay]
  2. queue._push(Request({
    validAfter: block.number + delay;
    encodedCall: encodedCall;
  }))
  ```

  

* `pop(address owner)`. Pop an update from the queue if any are pending, otherwise return an error.

  ```
  1. if len(queue) == 0, throw
  2. req = queue._pop() // unchecked pop
  3. if (block.number <= req.validAfter), StakeRegistry.call(req.encodedCall)
     else queue.push(req)
  ```

  When `pop` is called, first the queue is checked to see if there are any entries. If not, throw. Otherwise, check the type and block number of the first entry in the queue to see if the minimum delay for that entry has expired.

* `clear(address owner)`. Pop and discard all entries owned by `owner` from the queue and delete the queue from state. (The `migrateStake` endpoint needs to call this.)

* `committedStake(address owner)`. Calculate the `committed_stake` balance of `owner` that would be effected if all enqueued updates were applied.

* `checkNeighbourhood(address owner, bytes prefix)`. **OPTIONAL.** Check if `owner` has any pending updates that would reduce the population of the neighbourhood defined by `prefix`. This needs to be able to read the reported depth. Potentially an expensive operation which must cycle through all enqueued updates, but at least the user must pay for it.

When an `Update` is added to the queue, the round number of the current round is recorded along with it.

* The `Update` object records the call into the Staking contract that performs the metadata update.

  ```solidity
  struct Update {
    uint applicableFromBlock;
    bytes callData;
  }
  ```

* Under certain versions of the queue model, it is possible for the BZZ balance held by the Registry on behalf of an owner to be less than the balance registered for the Redistribution game. That is,

  * In the case of a deposit, BZZ transfer happens on request (?) while registered `potentialStake` is updated after a delay. (If we allow BZZ transfer to also occur after the delay, the depositor has a free option to cause the deposit to fail by moving his tokens somewhere else.)

  If this happens, then the updates that would register the untracked balances are in a queue and will eventually (after `BASE_UPDATE_DELAY` rounds) be processed.

* In the case of creating a new deposit, a new entry in the stake table may be created, but with zero balance, at the time the request is submitted. The balance is updated when the update is executed.

## Rationale

* *One queue.* All types of updates for all stake owners are considered to be part of one queue. While some queue designs may allow for handling different owners or different update types in isolation, others — such as a global churn rate limiter — require tracking global state. To future proof the queue interface against possible changes to queue design, other components of the system must treat the entire network-wide queue as a single black box.

* *Separate UpdateQueue contract.* We propose the update queue be maintained in a separate contract from the Stake Registry for the sake of maximising modularity and potentially isolating parts of deployments from unrelated upgrades.

  For the sake of gas efficiency, the UpdateQueue contract could be inlined into the StakeRegistry. However, we find this to be a premature optimisation that gives up modularity for the sake of gas fees that are basically insignificant (millionths of a dollar) in practice.

* *2 round thaw.* The 2 round thaw currently implemented (but not fully documented) in the `commit()` method of the Redistribution contract is absorbed into this queue. The delay length is preserved as the `BASE_UPDATE_DELAY` parameter. However, unlike in the old model, participation is still allowed during the thaw period — but under the old stake position.

* *Event emission.* Requesting updates should emit an event, because NOs will want to track these for their strategies. We don't need to emit an event when the update is actually applied. In any case, the time that the update is processed is not really economically significant; it just gets done whenever that node is next able to participate.

* *Per-neighbourhood delay scaling.* It may make sense to adjust the delay of changes depending on the before and after population of each neighbourhood affected by the change. The core example is to reduce delay for nodes leaving a neighbourhood with large population (and in the case of overlay change, entering one with small population). This would require the queue system to be able to estimate replication depth and enlarges the design space considerably, so we omit it from the present proposal.

* *Maximum queue length.* Although unlikely to be an issue in practice, in principle an update queue could grow so long that it cannot be emptied in a single block. Therefore, there shold be a maximum number of updates that can be held in the queue for each owner. It probably won't cause a big problem if the number is quite small, e.g. 10.

  An alternative approach would be to internally merge operations using an internal representation closed under composition. While we can imagine ways to do this for the set of operations the queue is currently expected to process, it would complicate the process of adding any new types of operation to the queue or changing the queue algorithm. A simple maximum queue length is easy to implement, universal, and unlikely to raise any serious objections.
  
* *Staker commitments.* Staker commitments, i.e. transfers to the stake registry and latest-price updates to committed stake, must be binding for the staker at the time the update is requested. The queue subsystem must be able to report up-to-date commitments. The effect of the new commitment (i.e. the new `committedStake` balance can be used in Redistribution) does not apply until after the delay.

* *Liability tracking.* The proposed changes mean that the `potentialStake` recorded under a given `owner` in the Stake Registry does not always equal the total amount of BZZ deposited by that owner (net of surplus withdrawals). Rather, the records of liabilities of the Stake Registry to a given owner are split between the Registry itself and the Update Queue. Since these records control what can be withdrawn by calling the `withdrawFromStake` and `migrateStake` methods, these processes must either block on not-yet-active updates, or fast track and apply them.

* *Manual queue triggering.* Manual popping of updates from the queue can be allowed, but since the updated metadata is only used during participation, there is not likely to be much incentive to do that. (This changes if reducing committed stake is allowed.)

### Effect of pending status on other components

* *Price oracle.* For the purposes of adjusting storage prices, the reveal counter should discount nodes currently waiting to exit the neighbourhood. Fully discounting it allows another node to enter and sync before the population shrinks without triggering negative price pressure. If no new node is ready to enter, this allows the system to begin adjusting prices from as soon as the potential supply shrinkage is known.

  It is natural to ask whether a node waiting to *enter* a different neighbourhood, for example by an overlay change, be considered as part of that neighbourhood for the sake of price adjustments. The current design means that we can only do this if nodes participate in the new neighbourhood while waiting in the queue, which would mean they need to be treated as having two overlay addresses for a period. Since reacting to neighbourhood underpopulation is more important than reacting to overpopulation, it may be OK to skip this one.

  This discounting could also be on a time-based curve, but that seems like it might be hard to implement.

* *Reward sharing.* For most of the benefits of an exit queue to work, nodes must be incentivised to continue operating while they are in the queue. Hence, they must be able to continue participating in reward sharing (and penalties) using their previous participation metadata while waiting. Accordingly, they must participate in all the activities that qualify them for reward sharing, i.e. reserve consensus and storage and density proofs.

* *Freezing.* If a node gets frozen while waiting to withdraw funds, what happens?

  * If withdrawal is allowed even if the stake is frozen at the end of the wait period, the penalty implied by freezing is effectively reduced gradually as the period nears its end.
  * If, on the other hand, frozen nodes cannot actually withdraw funds until the freeze period is ended, the freeze penalty has the effect of restricting access to capital. The fact that a withdrawal was attempted suggests that the value of being able to deploy that capital has recently become greater than the potential revenue, which is value of the freezing penalty under normal circumstances. Therefore it is not disproportionate for freezing to prevent withdrawal of funds if the freezing period would overlap the end of the `DRAWDOWN_DELAY` period.
  * Currently, frozen nodes are allowed to make deposits. Under the proposed queue system, funds are deposited at the time a deposit request is entered, but only registered for the purposes of redistribution after the delay `BASE_UPDATE_DELAY`. This only matters if the node participates, which it cannot if frozen. So the choice in this case is irrelevant.
  * If being frozen prevents or delays a node from executing an AoR change at the end of a period, it becomes harder to forecast node movements from queue state (because getting frozen screws that up). But that's the case with freezing anyway. Also, a frozen node cannot participate so its AoR is irrelevant.

  We therefore suggest that freezing be allowed to prevent the withdrawal of funds. All other changes have effect only during participation, which is anyway prevented during freezing.

  Can frozen nodes put in new update requests? I don't see why not.

* *Pausing.* When the Staking contract is paused, `migrateStake` is allowed and `manageStake` is not. Pausing the staking contract has no effect on participation in redistribution. The intention of this construction is to allow stake to move to a new version of the stake registry, so we see no reason to make `migrateStake` calls go via the queue. Instead, they should immediately clear and delete the queue, making sure to process all updates to liabilities in the form of `potentialStake`, and process the withdrawal.

### Concurrency

* If actions are anything other than instantaneous and atomic, we need to deal with concurrency — that is, an update being requested while another is waiting in the queue. 
* Different types of action ought to be treated differently, whence multiple delay types. 
* *In-order execution.* 
  * Insisting on in-order execution means that actions with short delays (e.g. topping up) can be held up by actions with longer delays (e.g. height reduction). This might not be necessary.
  * On the other hand, allowing out-of-order execution will probably make the analysis way more complicated. It will be harder to make use of the "forecast" function of the queue
* *Request cancellation.* Requires a way to specify which request should be cancelled, and again substantially complicates making use of the information benefits of a public queue. It is simpler and more elegant not to allow cancellations.

## Implementation notes

* Following standard practice, the event queue for each owner can be implemented with an address-indexed mapping. (Cf. [OpenZeppelin deque implementation](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/DoubleEndedQueue.sol) and a [Medium article](https://medium.com/@hayeah/diving-into-the-ethereum-vm-the-hidden-costs-of-arrays-28e119f04a9b) warning us about the use of arrays.)

## Security implications

* The update queue subsystem takes ownership, in the form of `BASE_UPDATE_DELAY`, of the 2 round metadata update delay currently found in the initial validation checks of the `Redistribution:commit()` call. A top-up or deposit delay of at least until the end of the current round is required to prevent shadow stake attacks. No immediate changes to security model for shadow stake or penalty evasion are implied by the current proposal, but care needs to be taken in future to preserve the `BASE_UPDATE_DELAY` minimum.
* In the proposed access control model, anyone may trigger processing of valid updates from anyone else's queue. Since updates cannot be cancelled and would be processed anyway before the state can be used in redistribution, this is harmless.

## Economic implications

The main effect, which is intended, is to slow down interactions with the stake registry, particularly those that could threaten data replication.

* The most serious threat to stability comes from height reduction, which removes a node entirely from the service of a particular neighbourhood. Incentives to reduce height may include:
  * Save on storage costs by reducing commitment.
  * Maintain stake density after a drawdown.
* We expect that the incentives for drawing down stake occur frequently, driven by market conditions and the attractiveness of other opportunities. Currently, the opportunities to withdraw stake are limited to when the storage price quote has gone down from when the stake was last "committed." Since only "uncommitted" stake can be withdrawn, withdrawing it has no immediate impact on the incentive to continue providing good service on the node, so no delay is needed.
* Changing overlay address does not affect the mean replication rate, but it weakens one neighbourhood while strengthening another. The design of the revenue sharing system implies that the incentives will often be for nodes to move from more populated neighbourhoods to less populated ones, but this need not always be the case. 
  Introducing a modest delay gives the network time to react to such changes, for example by migrating nodes from other neighbourhoods to fill a gap. Discounting exiting nodes from the replication rate counter of the source neighbourhood allows new nodes to enter without triggering downward price pressure.

## Interactions with other proposals

* *Self-custodial/upgradable stake registry.* This change would retire the `migrateStake` endpoint and possibly separate balance and participation metadata management into different contracts.

  When a change to the queue design occurs, metadata updates already waiting in the queue should ideally continue be processed under the old queue logic. If the queue state is part of the Stake Registry contract, there is no way to protect it from arbitrary updates. Thus the queue ought to be part of a new contract accessible by the Redistributor.

  If a self-custodial vault model is used to protect user actions from malicious registry upgrades, a separate Queue contract could facilitate protection of withdrawals by taking over a claim on the funds marked for withdrawal from the Registry, before ultimately returning it to the owner when the withdrawal is ready. It would then be impossible for a Registry upgrade to affect the winding down of the claim.

* *Withdrawable stake.* Withdrawing stake completely needs its own delay, at least as severe as for reducing height. Since it doesn't really make sense for exiting to be triggered by a call to `Redistribution:commit()`, there should be a separate endpoint to manually trigger exits.

  If stake is withdrawable under more general circumstances, we expect that freezing will prevent such withdrawals.
  
* *Automatic address balancing.* Current versions of automatic neighbourhood assignment call for a delayed commit/execute scheme to be allocated a neighbourhood after staking. The present update queue proposal provides a subsystem to implement this delay.